# DataStreamAPI（1.7版）
## 执行流程
- 设置执行环境
- 读取输入流
- 应用转换
- 输出结果
- 执行

Flink程序都是通过延迟计算（lazily execute）的方式执行。也就是说，那些创建数据源和转换操作的API调用不会立即触发数据处理，而只会在执行环境中构建
一个执行计划。计划中包含了从环境创建的流式数据源以及应用于这些数据源之上的一系列转换。只有在调用execute()方法时，系统才会触发程序执行。

构建完的JobGraph并提交到JobManager执行。根据执行环境类型的不同，系统可能需要将JobGraph发送到作为本地线程启动的JobManager上，也可能会将其发送
到远程的JobManager上。如果是后者，除JobGraph之外，我们还需要同时提供包含应用所需全部类和依赖的JAR包。

## 转换操作
### 完成一个DataStream API程序在本质上可以归结为：通过组合不同的转换来创建一个满足应用逻辑的Dataflow图。

将DataStream API的转换分为四类：
- 作用于单个事件的基本转换
- 针对相同键值事件的KeyedStream转换
- 将多条数据流合并为一条或将一条数据流拆分成多条流的转换
- 对流中的事件进行重新组织的分发转换

### 基本转换
#### Map
通过调用DataStream.map()方法可以指定map转换产生一个新的DataStream。

MapFunction的两个类型参数分别是输入事件和输出事件的类型，它们可以通过MapFunction接口来指定。该接口的map()方法将每个输入事件转换为一个输出事件。

#### Filter
Filter转换利用一个作用在流中每条输入事件上的布尔条件来决定事件的去留，如果返回值为true，那么它会保留输入事件并将其转发到输出，否则它会把事件丢弃。
通过调用DataStream.filter()方法可以指定filter转换产生一个数据类型不变的DataStream。

#### FlatMap
flatMap转换类似于Map，但它可以对每个输入事件产生零个、一个或多个输出事件。

### 基于KeyedStream的转换
作为DataStream API中一类特殊的DataStream，KeyedStream抽象可以从逻辑上将事件按照键值分配到多条独立的子流中。

作用于KeyedStream的状态化转换可以对当前处理事件的键值所对应上下文中的状态进行读写。这意味着所有键值相同的事件可以访问相同的状态。

#### keyBy
keyBy转换通过指定键值的方式将一个DataStream转化为KeyedStream。流中的事件会根据各自键值被分到不同的分区，这样一来，有着相同键值的事件一定会在后
续算子的同一个任务上处理。虽然键值不同的事件也可能会在同一个任务上处理，但任务函数所能访问的键值分区状态始终会被约束在当前事件键值的范围内。

#### 滚动聚合
滚动聚合转换作用于KeyedStream上，它将生成一个包含聚合结果（例如 求和、最小值、最大值等）的DataStream。滚动聚合算子会对每一个遇到过的键值保存一
个聚合结果。每当有新事件到来，该算子都会更新相应的聚合结果，并将其以事件的形式发送出去。

DataStream API中提供了以下滚动聚合方法：
- sum()：滚动计算输入流中指定字段的和
- min()：滚动计算输入流中指定字段的最小值
- max()：滚动计算输入流中指定字段的最大值
- minBy()：滚动计算输入流中迄今为止最小值，返回该值所在事件
- maxBy()：滚动计算输入流中迄今为止最大值，返回该值所在事件

> Tips：无法将多个滚动聚合方法组合使用，每次只能计算一个

#### Reduce
reduce转换是滚动聚合转换的泛化。它将一个ReduceFunction应用在一个KeyedStream中，每个到来的事件都会和reduce结果进行一次组合，从而产生一个新的
DataStream。reduce转换不会改变数据类型，因此输出流的类型会永远和输入流保持一致。

### 多流转换
#### Union
DataStream.union() 方法可以合并两条或多条类型相同的DataStream，生成一个新的类型相同的DataStream

union执行过程中，来自两条流的事件会以FIFO（先进先出）的方式合并，其顺序无法得到任何保证。此外，union算法不会对数据进行去重，每个输入消息都会被发往
下游算子。

#### Connect、coMap、coFlatMap
DataStream.connect()方法接收一个DataStream并返回一个ConnectedStream对象，该对象表示两个联结起来（connected）的流。

两个函数都是以两条输入流的类型外加输出流的类型作为其类型参数。它们为两条输入流定义了各自的处理方法。
```java
// IN1：第一条输入流的类型
// IN2：第二条输入流的类型
// OUT：输出元素的类型
CoMapFunction[IN1,IN2,OUT]
> Map1(IN1):OUT
> Map2(IN2):OUT
> FlatMap(IN1,Collector[OUT]): Unit
> FlatMap(IN2,Collector[OUT]): Unit
```

默认情况下，connect()方法不会使两条输入流的事件之间产生任何关联，因此所有事件都会随机分配给算子实例。该行为会产生不确定的结果，为了在ConnectedStream
上实现确定性的转换，connect()可以与keyby()和broadcast()结合使用。

无论是对ConnectedStreams执行keyby()还是对两个已经按键值分好区的数据流执行connect()，connect()转换都会将两个数据流中具有相同键值的事件发往
同一个算子实例上。

#### Split和Select
split转换是union转换的逆操作，它将输入流分割成两条或多条类型和输入流相同的输出流。每一个到来的事件都可以被发往零个、一个或多个输出流。因此，split
也可以用来过滤和复制事件。

DataStream.split()方法接收一个OutputSelector，它用来定义如何将数据流的元素分配到不同的命名输出（named output）中。OutputSelector中定义
的select()方法会在每个输入事件到来时被调用，并随即返回一个java.lang.Iterable[String]对象，针对某记录返回的一系列String值指定了该记录需要被
发往哪些输出流。

DataStream.split()方法会返回一个SplitStream对象，它提供的select()方法可以让我们通过指定输出名称的方式从SplitStream中选择一条或多条流。
> split转换限制了所有输出流的类型必须和输入流相同

### 分发转换
#### 随机
利用DataStream.shuffle()方法实现随机数据交换策略。该方法会依照均匀分布随机地记录发往后继算子的并行任务

#### 轮流
rebalance() 方法会将输入流中的事件以轮流方式均匀分配给后继任务

#### 重调
rescale() 也会以轮流方式对事件进行分发，但分发目标仅限于部分后继任务。本质上看，重调分区策略为发送端和接收端任务不等的情况提供了一种轻量级的负载均衡
方法，

rebalance() 和 rescale() 的本质不同体现在生成任务连接的方式。rebalance()会在所有发送任务和接收任务之间建立通信通道，而rescale()中每个发送
任务只会和下游算子的部分任务建立通道。

#### 广播
broadcast() 方法会将输入流中的事件复制并发往所有下游算子的并行任务。

#### 全局
global() 方法会将输入流中的所有事件发往下游算子的第一个并行任务。

#### 自定义
利用partitionCustion()方法自己定义分区策略。该方法接收一个Partitioner对象，在其中可以实现分区逻辑，定义分区需要参照字段或键值位置。

### 设置并行度
Flink应用可以在分布式环境中（例如 机器集群）并行执行。当提交一个DataStream程序到JobManager上执行时，系统会生成一个Dataflow图并准备好用于执行
的算子。每个算子都会产生一个或多个并行任务。每个任务负责处理算子的部分输入流。算子并行化任务的数目称为该算子的并行度。它决定了算子处理的并行化程度以及
能够处理的数据规模。

如果应用是在一个本地执行环境中执行，并行度会设置为CPU的线程数目。如果应用是提交到Flink集群运行，那么除非提交客户端明确指定，否则环境并行度将设置为
集群默认并行度

### 类型
#### 支持的数据类型
Flink支持的Java和Scala中所有常见数据类型，使用最多的可以分为以下几类：
- 原始类型
- Java和Scala元组
- Scala样例类
- POJO（包括Apache AVRO生成的类）
- 一些特殊类型

#### 原始类型
Flink支持Java和Scala的原始类型，Flink提供了Java元组的高效实现，它最多可包含25个字段，每个字段长度都对应一个单独的实现类——Tuple1 .. Tuple25，
这些元组类型都是强类型的

#### POJO
Flink会分析那些不属于任何一类的数据类型，并尝试将它们作为POJO类型进行处理。如果一个类满足如下条件，Flink就会将它看作POJO：
- 是一个公有类
- 有一个公有的五参默认构造函数
- 所有字段都是公有的，或提供了getter和setter方法
- 所有字段类型都必须是Flink所支持的

Flink还会将Avro自动生成的类作为POJO处理

### 为数据类型创建类型信息
Flink类型系统的核心是 TypeInformation，它为系统生成序列化器和比较器提供了必要的信息。

当应用提交执行时，Flink的类型系统会为将来所需处理的每种类型自动推断TypeInformation。一个名为类型提取器的组件会分析所有函数的泛型类型及返回类型，
以获取响应的TypeInformation对象。

Flink为Java和Scala提供了两个辅助类，其中的静态方法可以用来生成TypeInformation。
- Java：org.apache.flink.api.common.typeInfo.Types
- Scala：org.apache.flink.api.scala.typeutils.Types

### 显式提供类型信息
提供TypeInformation的方法有两种：
- 通过实现ResultTypeQueryable接口来扩展函数，在其中提供返回类型的TypeInformation
- 在定义Dataflow时候使用Java DataStream API中的returns 方法来显式指定某算子的返回类型

### 定义键值和引用字段
#### 字段位置
针对元组类型，可以简单使用元组相应元素的字段位置来定义键值。

#### 字段表达式
使用基于字段表达式，可以用于元组、POJO以及样例类，同时还支持选择嵌套的字段

如需选择POJO和元组中嵌套字段，可以利用"."来区分嵌套级别。

Flink还支持在混合类型上嵌套表达式。

可以通过通配符字段表达式"_"选择全部字段

### 键值选择器
第三种指定键值的方法是使用KeySelector函数，它可以从输入事件中提取键值。

KeySelector函数接收一个输入项，返回一个键值。这个键值不但可以是输入事件中的一个字段，还可以是经由任意计算得来的。

和使用字段位置以及字段表达式相比，KeySelector函数的一大好处是它返回的键值都是强类型的，因为KeySelector类需要提供泛型函数。

### 实现函数
#### 函数类
Flink中所有用户自定义函数（UDF）的接口都是以接口或抽象类的形式对外暴露

> 函数必须是Java可序列化的
> Flink会利用Java序列化机制将所有函数对象序列化后发送到对应的工作进程。用户函数中的全部内容都必须是可序列化的。

#### Lambda函数
lambda函数可用于Scala和Java

### 富函数
DataStreamAPI中所有的转换函数都有对应的富函数。富函数的使用位置和普通函数以及Lambda函数相同。富函数的命名规则都是以Rick开头。

在使用富函数的时候，可以对应函数的生命周期实现两个额外的方法：
- open() 是富函数中的初始化方法。它在每个任务首次调用转换方法前调用一次。open()通常用于那些只需进行一次的设置工作。注意，Configuration参数只在
DataStream API中使用而并没有在DataStreamAPI中用到。
- close() 作为函数的终止方法，会在每个任务最后一次调用转换方法后调用一次。它通常用于清理和释放资源。

### 导入外部和Flink依赖
- 将全部依赖打进应用的JAR包
- 可以将依赖的JAR包放到设置Flink的./lib目录中。


